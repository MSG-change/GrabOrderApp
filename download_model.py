#!/usr/bin/env python3
"""
ä¸‹è½½æ¨¡åž‹æ–‡ä»¶è„šæœ¬
ç”±äºŽæ¨¡åž‹æ–‡ä»¶å¤ªå¤§(137MB)ï¼Œæ— æ³•æäº¤åˆ°GitHub
ä½¿ç”¨æ­¤è„šæœ¬ä»Žäº‘ç«¯ä¸‹è½½
"""

import os
import requests
from tqdm import tqdm

def download_model():
    """ä¸‹è½½æ¨¡åž‹æ–‡ä»¶"""
    
    # æ¨¡åž‹æ–‡ä»¶URLï¼ˆéœ€è¦æ›¿æ¢ä¸ºå®žé™…çš„ä¸‹è½½åœ°å€ï¼‰
    # å¯ä»¥ä½¿ç”¨ï¼š
    # 1. Google Drive
    # 2. ç™¾åº¦ç½‘ç›˜
    # 3. é˜¿é‡Œäº‘OSS
    # 4. GitHub Releases
    # 5. è‡ªå»ºæœåŠ¡å™¨
    
    MODEL_URL = "https://your-server.com/best_siamese_model.pth"  # æ›¿æ¢ä¸ºå®žé™…åœ°å€
    MODEL_PATH = "best_siamese_model.pth"
    MODEL_SIZE = 144114997  # 137.44 MB
    
    if os.path.exists(MODEL_PATH):
        print(f"âœ… æ¨¡åž‹æ–‡ä»¶å·²å­˜åœ¨: {MODEL_PATH}")
        file_size = os.path.getsize(MODEL_PATH)
        if file_size == MODEL_SIZE:
            print(f"   æ–‡ä»¶å¤§å°æ­£ç¡®: {file_size/1024/1024:.2f} MB")
            return True
        else:
            print(f"âš ï¸  æ–‡ä»¶å¤§å°ä¸åŒ¹é…: {file_size} != {MODEL_SIZE}")
            print(f"   é‡æ–°ä¸‹è½½...")
    
    print(f"ðŸ“¥ ä¸‹è½½æ¨¡åž‹æ–‡ä»¶...")
    print(f"   URL: {MODEL_URL}")
    print(f"   å¤§å°: {MODEL_SIZE/1024/1024:.2f} MB")
    
    try:
        response = requests.get(MODEL_URL, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        
        with open(MODEL_PATH, 'wb') as f:
            with tqdm(total=total_size, unit='B', unit_scale=True, desc="ä¸‹è½½è¿›åº¦") as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))
        
        print(f"âœ… ä¸‹è½½å®Œæˆ: {MODEL_PATH}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False


def check_model():
    """æ£€æŸ¥æ¨¡åž‹æ–‡ä»¶"""
    MODEL_PATH = "best_siamese_model.pth"
    
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨")
        print(f"   è¯·è¿è¡Œ: python download_model.py")
        return False
    
    file_size = os.path.getsize(MODEL_PATH)
    print(f"âœ… æ¨¡åž‹æ–‡ä»¶å­˜åœ¨")
    print(f"   è·¯å¾„: {MODEL_PATH}")
    print(f"   å¤§å°: {file_size/1024/1024:.2f} MB")
    
    # å°è¯•åŠ è½½æ¨¡åž‹éªŒè¯
    try:
        import torch
        checkpoint = torch.load(MODEL_PATH, map_location='cpu')
        if 'model_state_dict' in checkpoint:
            print(f"   å‡†ç¡®çŽ‡: {checkpoint.get('val_acc', 0)*100:.2f}%")
        print(f"âœ… æ¨¡åž‹æ–‡ä»¶æœ‰æ•ˆ")
        return True
    except Exception as e:
        print(f"âŒ æ¨¡åž‹æ–‡ä»¶æ— æ•ˆ: {e}")
        return False


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'check':
        check_model()
    else:
        download_model()
