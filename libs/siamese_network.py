try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import Dataset, DataLoader
    from torchvision import transforms, models
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸  PyTorch not available in siamese_network.py")
    # åˆ›å»ºdummyç±»ä»¥é¿å…é”™è¯¯
    class Dataset:
        pass
    class nn:
        class Module:
            pass

from PIL import Image
import numpy as np
import random

if TORCH_AVAILABLE:
    BaseDataset = Dataset
    BaseModule = nn.Module
else:
    BaseDataset = object
    BaseModule = object

class SiameseDataset(BaseDataset):
    """å­ªç”Ÿç½‘ç»œæ•°æ®é›†ç±»"""
    
    def __init__(self, dataset_path: str, transform=None, mode='train', train_ratio=0.8, random_seed=42):
        """
        Args:
            dataset_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
            transform: å›¾åƒå˜æ¢
            mode: 'train' æˆ– 'test' æˆ– 'val'
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            random_seed: éšæœºç§å­ï¼Œç¡®ä¿æ•°æ®é›†åˆ’åˆ†å¯å¤ç°
        """
        self.dataset_path = dataset_path
        self.transform = transform
        self.mode = mode
        
        # è·å–æ‰€æœ‰æ–‡ä»¶å¤¹ï¼ˆæ’åºåå†shuffleï¼Œç¡®ä¿å¯å¤ç°ï¼‰
        self.folders = sorted([f for f in os.listdir(dataset_path) 
                              if os.path.isdir(os.path.join(dataset_path, f))])
        
        # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡åˆ’åˆ†ç›¸åŒ
        random.seed(random_seed)
        random.shuffle(self.folders)
        random.seed()  # é‡ç½®éšæœºç§å­
        
        # æŒ‰æ¯”ä¾‹åˆ†å‰²æ•°æ®é›†
        split_idx = int(len(self.folders) * train_ratio)
        val_split = int(split_idx * 0.8)  # è®­ç»ƒé›†çš„80%ç”¨äºè®­ç»ƒï¼Œ20%ç”¨äºéªŒè¯
        
        if mode == 'train':
            self.folders = self.folders[:val_split]
        elif mode == 'val':
            self.folders = self.folders[val_split:split_idx]
        elif mode == 'test':
            self.folders = self.folders[split_idx:]
        
        print(f"{mode.upper()}é›†åŒ…å« {len(self.folders)} ä¸ªæ ·æœ¬æ–‡ä»¶å¤¹")
        
    def __len__(self):
        # æ¯ä¸ªæ–‡ä»¶å¤¹ç”Ÿæˆå¤šä¸ªæ­£è´Ÿæ ·æœ¬å¯¹
        return len(self.folders) * 20  # æ¯ä¸ªæ–‡ä»¶å¤¹ç”Ÿæˆ20ä¸ªæ ·æœ¬å¯¹
    
    def __getitem__(self, idx):
        """ç”Ÿæˆä¸€ä¸ªæ ·æœ¬å¯¹å’Œæ ‡ç­¾"""
        folder_idx = idx // 20
        folder_name = self.folders[folder_idx]
        folder_path = os.path.join(self.dataset_path, folder_name)
        
        # è¯»å–é—®é¢˜å›¾ç‰‡
        question_path = os.path.join(folder_path, 'question.png')
        if not os.path.exists(question_path):
            # å¦‚æœæ²¡æœ‰question.pngï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªå›¾ç‰‡ä½œä¸ºanchor
            all_images = glob.glob(os.path.join(folder_path, '*.png'))
            question_path = random.choice(all_images)
        
        question_img = Image.open(question_path).convert('RGB')
        
        # ä»annotation.jsonè¯»å–æ­£ç¡®ç­”æ¡ˆçš„ç´¢å¼•
        import json
        annotation_path = os.path.join(folder_path, 'annotation.json')
        answer_indices = []
        
        if os.path.exists(annotation_path):
            try:
                with open(annotation_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    answer_indices = metadata.get('answers', [])
            except:
                pass
        
        # æ ¹æ®annotation.jsonä¸­çš„ç´¢å¼•æ„å»ºæ­£è´Ÿæ ·æœ¬
        if answer_indices:
            # æ ¹æ®ç´¢å¼•æ„å»ºæ­£è´Ÿæ ·æœ¬æ–‡ä»¶è·¯å¾„
            positive_files = [os.path.join(folder_path, f'geetest_{i}.png') 
                            for i in answer_indices if i < 9]
            negative_indices = [i for i in range(9) if i not in answer_indices]
            negative_files = [os.path.join(folder_path, f'geetest_{i}.png') 
                            for i in negative_indices]
            # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
            positive_files = [f for f in positive_files if os.path.exists(f)]
            negative_files = [f for f in negative_files if os.path.exists(f)]
        else:
            # å¦‚æœæ²¡æœ‰annotation.jsonï¼Œfallbackåˆ°è¯»å–answeræ–‡ä»¶
            answer_files = glob.glob(os.path.join(folder_path, 'geetest_answer_*.png'))
            if answer_files:
                # ä½¿ç”¨answeræ–‡ä»¶
                positive_files = answer_files
                # æ‰€æœ‰å€™é€‰å›¾ç‰‡
                all_candidates = [os.path.join(folder_path, f'geetest_{i}.png') for i in range(9)]
                negative_files = [f for f in all_candidates if os.path.exists(f)]
            else:
                positive_files = []
                negative_files = []
        
        # éšæœºå†³å®šç”Ÿæˆæ­£æ ·æœ¬å¯¹è¿˜æ˜¯è´Ÿæ ·æœ¬å¯¹
        is_positive = random.random() > 0.5
        
        if is_positive and positive_files:
            # ç”Ÿæˆæ­£æ ·æœ¬å¯¹
            pair_path = random.choice(positive_files)
            label = 1
        elif negative_files:
            # ç”Ÿæˆè´Ÿæ ·æœ¬å¯¹
            pair_path = random.choice(negative_files)
            label = 0
        else:
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼Œfallback
            all_images = glob.glob(os.path.join(folder_path, '*.png'))
            all_images = [img for img in all_images if img != question_path and 'question' not in img]
            if all_images:
                pair_path = random.choice(all_images)
                label = 1 if 'answer' in pair_path else 0
            else:
                # æœ€åçš„fallback - ä½¿ç”¨questionè‡ªå·±ä½œä¸ºæ­£æ ·æœ¬
                pair_path = question_path
                label = 1
        
        pair_img = Image.open(pair_path).convert('RGB')
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            question_img = self.transform(question_img)
            pair_img = self.transform(pair_img)
        
        return question_img, pair_img, torch.tensor(label, dtype=torch.float32)


class ResNetBackbone(BaseModule):
    """ResNetç‰¹å¾æå–å™¨"""
    
    def __init__(self, pretrained=True, feature_dim=512):
        if TORCH_AVAILABLE:
            super(ResNetBackbone, self).__init__()
        
        # ä½¿ç”¨é¢„è®­ç»ƒçš„ResNet-18
        self.resnet = models.resnet18(pretrained=pretrained)
        
        # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])
        
        # æ·»åŠ ç‰¹å¾é™ç»´å±‚
        self.feature_projector = nn.Sequential(
            nn.Linear(512, feature_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(feature_dim, feature_dim)
        )
        
    def forward(self, x):
        # æå–ç‰¹å¾
        features = self.resnet(x)
        features = features.view(features.size(0), -1)  # å±•å¹³
        
        # ç‰¹å¾æŠ•å½±
        features = self.feature_projector(features)
        
        # L2å½’ä¸€åŒ–
        features = F.normalize(features, p=2, dim=1)
        
        return features


class SiameseNetwork(BaseModule):
    """å­ªç”Ÿç¥ç»ç½‘ç»œ"""
    
    def __init__(self, feature_dim=512):
        if TORCH_AVAILABLE:
            super(SiameseNetwork, self).__init__()
        
        # å…±äº«çš„ç‰¹å¾æå–å™¨
        self.backbone = ResNetBackbone(pretrained=True, feature_dim=feature_dim)
        
        # ç›¸ä¼¼åº¦è®¡ç®—å±‚ï¼ˆè¾“å‡ºlogitsï¼Œä¸ä½¿ç”¨sigmoidï¼‰
        # é…åˆBCEWithLogitsLossä½¿ç”¨ï¼Œæ•°å€¼æ›´ç¨³å®š
        self.similarity_head = nn.Sequential(
            nn.Linear(feature_dim * 2, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1)
            # æ³¨æ„ï¼šä¸ä½¿ç”¨Sigmoidï¼Œè¾“å‡ºlogitsç»™BCEWithLogitsLoss
        )
        
    def forward(self, img1, img2):
        # æå–ä¸¤ä¸ªå›¾åƒçš„ç‰¹å¾
        feat1 = self.backbone(img1)
        feat2 = self.backbone(img2)
        
        # è®¡ç®—ç‰¹å¾è·ç¦»ï¼ˆå¯é€‰æ–¹æ³•ï¼‰
        euclidean_distance = F.pairwise_distance(feat1, feat2)
        cosine_similarity = F.cosine_similarity(feat1, feat2)
        
        # è¿æ¥ç‰¹å¾è¿›è¡Œç›¸ä¼¼åº¦é¢„æµ‹
        combined_features = torch.cat([feat1, feat2], dim=1)
        similarity_score = self.similarity_head(combined_features)
        
        # åªsqueezeæœ€åä¸€ä¸ªç»´åº¦ [batch, 1] -> [batch]
        return similarity_score.squeeze(-1), euclidean_distance, cosine_similarity


class ContrastiveLoss(nn.Module):
    """å¯¹æ¯”æŸå¤±å‡½æ•°"""
    
    def __init__(self, margin=1.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, distance, label):
        # å¯¹æ¯”æŸå¤±è®¡ç®—
        loss = torch.mean(
            label * torch.pow(distance, 2) + 
            (1 - label) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)
        )
        return loss


class TripletLoss(nn.Module):
    """ä¸‰å…ƒç»„æŸå¤±å‡½æ•°"""
    
    def __init__(self, margin=0.3):
        super(TripletLoss, self).__init__()
        self.margin = margin
        
    def forward(self, anchor, positive, negative):
        distance_positive = F.pairwise_distance(anchor, positive)
        distance_negative = F.pairwise_distance(anchor, negative)
        
        loss = torch.mean(torch.clamp(
            distance_positive - distance_negative + self.margin, min=0.0))
        
        return loss


def get_transforms():
    """è·å–æ•°æ®å˜æ¢"""
    
    if not TORCH_AVAILABLE:
        # æ— torchæ—¶è¿”å›None
        return None, None
    
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(p=0.2),
        transforms.RandomRotation(degrees=5),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    test_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    return train_transform, test_transform


def train_model(model, train_loader, val_loader, num_epochs=50, device='cuda'):
    """è®­ç»ƒæ¨¡å‹"""
    import time
    import sys
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)
    
    # ä½¿ç”¨BCEWithLogitsLossï¼Œæ•°å€¼æ›´ç¨³å®šï¼ˆæ¨¡å‹è¾“å‡ºlogitsï¼‰
    bce_loss = nn.BCEWithLogitsLoss()
    contrastive_loss = ContrastiveLoss(margin=1.0)
    
    best_val_acc = 0.0
    train_losses = []
    val_accuracies = []
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0.0
        running_loss = 0.0
        start_time = time.time()
        
        for batch_idx, (img1, img2, labels) in enumerate(train_loader):
            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            similarity_scores, euclidean_dist, cosine_sim = model(img1, img2)
            
            # è®¡ç®—æŸå¤±ï¼ˆç»“åˆå¤šç§æŸå¤±ï¼‰
            bce_loss_val = bce_loss(similarity_scores, labels)
            contrastive_loss_val = contrastive_loss(euclidean_dist, labels)
            
            # æ€»æŸå¤±
            total_loss_val = bce_loss_val + 0.5 * contrastive_loss_val
            
            # åå‘ä¼ æ’­
            total_loss_val.backward()
            optimizer.step()
            
            # æ›´æ–°æŸå¤±ç»Ÿè®¡
            total_loss += total_loss_val.item()
            running_loss += total_loss_val.item()
            
            # è®¡ç®—è¿›åº¦
            progress = (batch_idx + 1) / len(train_loader)
            bar_length = 40
            filled_length = int(bar_length * progress)
            bar = 'â–ˆ' * filled_length + 'â–’' * (bar_length - filled_length)
            
            # è®¡ç®—é€Ÿåº¦å’ŒETA
            elapsed_time = time.time() - start_time
            batches_per_sec = (batch_idx + 1) / elapsed_time if elapsed_time > 0 else 0
            eta_seconds = (len(train_loader) - batch_idx - 1) / batches_per_sec if batches_per_sec > 0 else 0
            eta_str = f"{int(eta_seconds//60):02d}:{int(eta_seconds%60):02d}"

            # æ˜¾ç¤ºè¿›åº¦
            avg_loss = running_loss / (batch_idx + 1)
            sys.stdout.write(f'\rEpoch {epoch+1}/{num_epochs} |{bar}| '
                           f'{batch_idx+1}/{len(train_loader)} '
                           f'[{progress*100:5.1f}%] '
                           f'loss: {avg_loss:.4f} '
                           f'ETA: {eta_str} '
                           f'{batches_per_sec:.1f}it/s')
            sys.stdout.flush()
        
        # æ¢è¡Œå¹¶æ˜¾ç¤ºepochæ€»ç»“
        print()
        avg_train_loss = total_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # éªŒè¯é˜¶æ®µ
        print("Validating...")
        val_acc = evaluate_model(model, val_loader, device, verbose=False)  # éªŒè¯æ—¶ä¸æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        val_accuracies.append(val_acc)
        
        # epochæ€»ç»“
        epoch_time = time.time() - start_time
        lr = optimizer.param_groups[0]['lr']
        print(f'Epoch {epoch+1}/{num_epochs}: '
              f'train_loss={avg_train_loss:.4f} '
              f'val_acc={val_acc:.4f} '
              f'lr={lr:.6f} '
              f'time={epoch_time:.1f}s')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_siamese_model.pth')
            print(f'ğŸ† New best model saved! Val Acc: {best_val_acc:.4f}')
        
        scheduler.step()
        print("-" * 80)
    
    return train_losses, val_accuracies


def evaluate_model(model, test_loader, device='cuda', verbose=True):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for img1, img2, labels in test_loader:
            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)
            
            # æ¨¡å‹è¾“å‡ºlogitsï¼Œéœ€è¦sigmoidè½¬æ¢ä¸ºæ¦‚ç‡
            logits, _, _ = model(img1, img2)
            similarity_probs = torch.sigmoid(logits)
            predictions = (similarity_probs > 0.5).float()
            
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—åŸºæœ¬å‡†ç¡®ç‡
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    accuracy = np.mean(all_predictions == all_labels)
    
    if verbose:
            print(f'å‡†ç¡®ç‡: {accuracy:.4f}')
            # æ‰‹åŠ¨è®¡ç®—ç²¾ç¡®ç‡å’Œå¬å›ç‡
            tp = np.sum((all_predictions == 1) & (all_labels == 1))
            fp = np.sum((all_predictions == 1) & (all_labels == 0))
            fn = np.sum((all_predictions == 0) & (all_labels == 1))
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            print(f'ç²¾ç¡®ç‡: {precision:.4f}')
            print(f'å¬å›ç‡: {recall:.4f}')
            print(f'F1åˆ†æ•°: {f1:.4f}')
    
    return accuracy


def predict_similarity(model, img1_path, img2_path, transform, device='cuda'):
    """é¢„æµ‹ä¸¤å¼ å›¾ç‰‡çš„ç›¸ä¼¼åº¦"""
    model.eval()
    
    # åŠ è½½å›¾ç‰‡
    img1 = Image.open(img1_path).convert('RGB')
    img2 = Image.open(img2_path).convert('RGB')
    
    # åº”ç”¨å˜æ¢
    img1 = transform(img1).unsqueeze(0).to(device)
    img2 = transform(img2).unsqueeze(0).to(device)
    
    with torch.no_grad():
        # æ¨¡å‹è¾“å‡ºlogitsï¼Œéœ€è¦sigmoidè½¬æ¢ä¸ºæ¦‚ç‡
        logits, euclidean_dist, cosine_sim = model(img1, img2)
        similarity_score = torch.sigmoid(logits)
    
    return {
        'similarity_score': similarity_score.item(),
        'euclidean_distance': euclidean_dist.item(),
        'cosine_similarity': cosine_sim.item(),
        'is_similar': similarity_score.item() > 0.5
    }