#!/usr/bin/env python3
"""
ONNX Runtimeæ¨ç† - é€‚åˆAndroidè¿è¡Œ
è½»é‡çº§ï¼Œæ— éœ€PyTorch
"""

import numpy as np
from PIL import Image
import os
import json

try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    print("âš ï¸  ONNX Runtimeæœªå®‰è£…ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")


class ONNXInference:
    """ONNXæ¨¡å‹æ¨ç†å™¨ï¼ˆAndroidä¼˜åŒ–ï¼‰"""
    
    def __init__(self, model_path="siamese_model_quantized.onnx"):
        """
        åˆå§‹åŒ–ONNXæ¨ç†å™¨
        
        Args:
            model_path: ONNXæ¨¡å‹è·¯å¾„ï¼ˆå»ºè®®ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬ï¼‰
        """
        self.model_path = model_path
        self.session = None
        self.input_names = None
        self.output_names = None
        self.threshold = 0.5
        
        if ONNX_AVAILABLE:
            self._load_model()
        else:
            print("ğŸ“Œ ä½¿ç”¨å›ºå®šæ¨¡å¼ï¼ˆæ— ONNXï¼‰")
    
    def _load_model(self):
        """åŠ è½½ONNXæ¨¡å‹"""
        if not os.path.exists(self.model_path):
            print(f"âš ï¸  æ‰¾ä¸åˆ°ONNXæ¨¡å‹: {self.model_path}")
            return
        
        try:
            # åˆ›å»ºæ¨ç†ä¼šè¯ï¼ˆä¼˜åŒ–é€‰é¡¹ï¼‰
            options = ort.SessionOptions()
            options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # Androidä¼˜åŒ–ï¼šä½¿ç”¨CPUæä¾›å™¨
            providers = ['CPUExecutionProvider']
            
            self.session = ort.InferenceSession(
                self.model_path,
                sess_options=options,
                providers=providers
            )
            
            # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
            self.input_names = [inp.name for inp in self.session.get_inputs()]
            self.output_names = [out.name for out in self.session.get_outputs()]
            
            print(f"âœ… ONNXæ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   è¾“å…¥: {self.input_names}")
            print(f"   è¾“å‡º: {self.output_names}")
            
            # æ˜¾ç¤ºæ¨¡å‹å¤§å°
            model_size = os.path.getsize(self.model_path) / (1024 * 1024)
            print(f"   å¤§å°: {model_size:.2f} MB")
            
        except Exception as e:
            print(f"âŒ ONNXåŠ è½½å¤±è´¥: {e}")
            self.session = None
    
    def preprocess_image(self, image):
        """
        é¢„å¤„ç†å›¾ç‰‡ï¼ˆä¸PyTorchä¿æŒä¸€è‡´ï¼‰
        
        Args:
            image: PIL Imageæˆ–numpy array
        
        Returns:
            é¢„å¤„ç†åçš„numpy array
        """
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        
        # è°ƒæ•´å¤§å°
        image = image.resize((224, 224), Image.Resampling.LANCZOS)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        img_array = np.array(image).astype(np.float32)
        
        # å½’ä¸€åŒ–ï¼ˆImageNetæ ‡å‡†ï¼‰
        mean = np.array([0.485, 0.456, 0.406]) * 255
        std = np.array([0.229, 0.224, 0.225]) * 255
        img_array = (img_array - mean) / std
        
        # è½¬æ¢ç»´åº¦: HWC -> CHW
        img_array = np.transpose(img_array, (2, 0, 1))
        
        # æ·»åŠ batchç»´åº¦
        img_array = np.expand_dims(img_array, axis=0)
        
        return img_array.astype(np.float32)
    
    def predict(self, image1, image2):
        """
        é¢„æµ‹ä¸¤å¼ å›¾ç‰‡çš„ç›¸ä¼¼åº¦
        
        Args:
            image1: ç¬¬ä¸€å¼ å›¾ç‰‡ï¼ˆè·¯å¾„æˆ–PIL Imageï¼‰
            image2: ç¬¬äºŒå¼ å›¾ç‰‡ï¼ˆè·¯å¾„æˆ–PIL Imageï¼‰
        
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        if not ONNX_AVAILABLE or self.session is None:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šè¿”å›å›ºå®šç›¸ä¼¼åº¦
            return 0.6
        
        try:
            # é¢„å¤„ç†å›¾ç‰‡
            img1 = self.preprocess_image(image1)
            img2 = self.preprocess_image(image2)
            
            # å‡†å¤‡è¾“å…¥
            inputs = {
                self.input_names[0]: img1,
                self.input_names[1]: img2
            }
            
            # æ‰§è¡Œæ¨ç†
            outputs = self.session.run(self.output_names, inputs)
            
            # è·å–ç›¸ä¼¼åº¦ï¼ˆç¬¬ä¸€ä¸ªè¾“å‡ºï¼‰
            similarity_logits = outputs[0][0][0]
            
            # Sigmoidæ¿€æ´»
            similarity = 1 / (1 + np.exp(-similarity_logits))
            
            return float(similarity)
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            return 0.0
    
    def predict_batch(self, question_img, grid_cells):
        """
        æ‰¹é‡é¢„æµ‹ä¹å®«æ ¼
        
        Args:
            question_img: é¢˜ç›®å›¾ç‰‡
            grid_cells: ä¹å®«æ ¼å›¾ç‰‡åˆ—è¡¨
        
        Returns:
            é€‰ä¸­çš„æ ¼å­ç´¢å¼•åˆ—è¡¨
        """
        if not ONNX_AVAILABLE or self.session is None:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šé€‰æ‹©å‰3ä¸ª
            return [0, 1, 2]
        
        answers = []
        
        for idx, cell in enumerate(grid_cells):
            score = self.predict(question_img, cell)
            if score > self.threshold:
                answers.append(idx)
        
        # å¦‚æœæ²¡æœ‰é€‰ä¸­ï¼Œè‡³å°‘é€‰æ‹©å¾—åˆ†æœ€é«˜çš„3ä¸ª
        if not answers:
            scores = [(idx, self.predict(question_img, cell)) 
                     for idx, cell in enumerate(grid_cells)]
            scores.sort(key=lambda x: x[1], reverse=True)
            answers = [idx for idx, _ in scores[:3]]
        
        return answers


class AndroidOptimizedInference:
    """Androidä¼˜åŒ–çš„æ¨ç†å™¨ï¼ˆæ›´è½»é‡ï¼‰"""
    
    def __init__(self):
        """ä½¿ç”¨æœ€ç®€å•çš„ç­–ç•¥ï¼Œæ— éœ€æ¨¡å‹"""
        self.patterns = [
            [0, 1, 2],    # å‰3ä¸ª
            [0, 3, 6],    # å·¦åˆ—
            [0, 4, 8],    # å¯¹è§’çº¿
            [2, 4, 6],    # åå¯¹è§’çº¿
            [1, 4, 7],    # ä¸­é—´åˆ—
            [3, 4, 5],    # ä¸­é—´è¡Œ
        ]
        self.current_pattern = 0
    
    def predict_batch(self, question_img, grid_cells):
        """
        ä½¿ç”¨é¢„å®šä¹‰æ¨¡å¼é€‰æ‹©
        æ¯æ¬¡ä½¿ç”¨ä¸åŒçš„æ¨¡å¼å¢åŠ æˆåŠŸç‡
        """
        pattern = self.patterns[self.current_pattern % len(self.patterns)]
        self.current_pattern += 1
        return pattern


# å¯¼å‡ºä¾¿æ·å‡½æ•°
def get_inference_engine(use_onnx=True):
    """
    è·å–æ¨ç†å¼•æ“
    
    Args:
        use_onnx: æ˜¯å¦ä½¿ç”¨ONNXï¼ˆFalseæ—¶ä½¿ç”¨è½»é‡çº§æ–¹æ¡ˆï¼‰
    
    Returns:
        æ¨ç†å¼•æ“å®ä¾‹
    """
    if use_onnx and ONNX_AVAILABLE:
        return ONNXInference()
    else:
        return AndroidOptimizedInference()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    engine = get_inference_engine()
    print(f"ä½¿ç”¨å¼•æ“: {type(engine).__name__}")
